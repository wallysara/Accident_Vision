{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "56lwEQVts72M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1595d791-64d8-426c-b444-a85cfdd7e39c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from moviepy.editor import VideoFileClip\n",
        "from keras import layers, applications\n",
        "from datasets import load_dataset\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms.functional as F\n",
        "import torch.nn.functional as FF\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2, os, gc, torch, time"
      ],
      "metadata": {
        "id": "r5K5XLrUGMki",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
         },
        "id": "299ub9AbtTYA",
        "outputId": "6cf7fa47-ed25-4c90-f8ef-901e961d7370"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1502 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19cc8398acac49f3920cc8d8b8fb2cdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1348 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38cb0c5f4766446f9e4a25420ccafd13"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "while True :\n",
        "    try :\n",
        "        ds = load_dataset('nexar-ai/nexar_collision_prediction', split='train')\n",
        "        break\n",
        "\n",
        "    except :\n",
        "        time.sleep(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TwOr4Ad_tsC"
      },
      "outputs": [],
      "source": [
        "n = round(ds.num_rows * 0.5)\n",
        "subset = ds.shuffle(seed=1404).select(range(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdBa8Xtvtcs7"
      },
      "outputs": [],
      "source": [
        "train_df = subset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f11eX2VbE_T"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df['time_of_event'].apply(lambda x: 0 if pd.isna(x) or x is None else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbF_a676cyPI"
      },
      "outputs": [],
      "source": [
        "train_df = pd.get_dummies(\n",
        "    train_df,\n",
        "    columns=['light_conditions', 'weather', 'scene'],\n",
        "    prefix=['light', 'weather', 'scene'],\n",
        "    dtype=int\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_split, val_df_split = train_test_split(train_df, test_size=0.3, random_state=1404)"
      ],
      "metadata": {
        "id": "7781WD8Whpv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'video_matrices'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def resize_on_gpu(frames):\n",
        "    # Convert (N, H, W, C) -> (N, C, H, W)\n",
        "    frames_tensor = torch.from_numpy(frames).permute(0, 3, 1, 2).float().to(device)\n",
        "    # Resize to (224, 224)\n",
        "    resized = F.resize(frames_tensor, [224, 224])\n",
        "    # Back to (N, H, W, C)\n",
        "    return resized.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "def video_to_matrix(video_df, is_train=True, frame_interval=6):\n",
        "    for idx, row in tqdm(video_df.iterrows(), total=len(video_df)):\n",
        "        video_path = row['video']['path']\n",
        "        time_of_event = row['time_of_event']\n",
        "        label = row['label']\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f'[WARN] Cannot open video at index {idx}: {video_path}')\n",
        "            continue\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps == 0 or fps is None:\n",
        "            print(f'[WARN] FPS is zero at index {idx}')\n",
        "            cap.release()\n",
        "            continue\n",
        "\n",
        "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        duration = total_frames / fps\n",
        "\n",
        "        if label :\n",
        "            start_time = time_of_event - 2\n",
        "            end_time = time_of_event\n",
        "        else :\n",
        "            random_time = np.random.uniform(0, duration - 2)\n",
        "            start_time = random_time\n",
        "            end_time = random_time + 2\n",
        "\n",
        "        start_frame = int(start_time * fps)\n",
        "        end_frame = int(end_time * fps)\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        frames = []\n",
        "        frame_index = start_frame\n",
        "\n",
        "        while frame_index <= end_frame:\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            if frame_index % frame_interval == 0:\n",
        "                frames.append(frame)\n",
        "            frame_index += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if frames:\n",
        "            frames = np.stack(frames)  # (N, H, W, 3)\n",
        "            video_matrix = resize_on_gpu(frames)  # GPU accelerated resize\n",
        "            save_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
        "            np.save(save_path, video_matrix)\n",
        "\n",
        "        del frames\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# Run it\n",
        "video_to_matrix(train_df_split)\n",
        "video_to_matrix(val_df_split, is_train=False)"
      ],
      "metadata": {
        "id": "81feoejPGdHd",
        "outputId": "931d53ea-3bc5-41e0-cea0-072d354bdb97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 525/525 [13:01<00:00,  1.49s/it]\n",
            "100%|██████████| 225/225 [05:24<00:00,  1.44s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_tHhlu4acKv"
      },
      "outputs": [],
      "source": [
        "def add_matrix_path_col(df) :\n",
        "  output_dir = 'video_matrices'\n",
        "  df['video_matrix_path'] = None\n",
        "\n",
        "  for idx in df.index:\n",
        "      file_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
        "      if os.path.exists(file_path):\n",
        "          df.at[idx, 'video_matrix_path'] = file_path\n",
        "      else:\n",
        "          df.at[idx, 'video_matrix_path'] = None\n",
        "\n",
        "add_matrix_path_col(train_df_split)\n",
        "add_matrix_path_col(val_df_split)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_video_to_matrix(df):\n",
        "    output_dir = 'masked_video_matrices'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    for idx, video in tqdm(df.iterrows(), total=len(df)):\n",
        "        video_path = '/content/' + video['video_matrix_path']\n",
        "        video_frames = np.load(video_path)  # (N, H, W, 3)\n",
        "        frames_tensor = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
        "        num_frames = frames_tensor.shape[0]\n",
        "\n",
        "        mask_frames = []\n",
        "\n",
        "        # Convert to grayscale on GPU\n",
        "        gray = 0.2989 * frames_tensor[:, 0] + 0.5870 * frames_tensor[:, 1] + 0.1140 * frames_tensor[:, 2]\n",
        "\n",
        "        for i in range(1, num_frames):\n",
        "            prev_gray = gray[i - 1:i]  # (1, H, W)\n",
        "            next_gray = gray[i:i + 1]\n",
        "\n",
        "            # === Optical flow approximation using spatial gradients ===\n",
        "            # Compute gradients with Sobel kernels\n",
        "            sobel_x = torch.tensor([[1, 0, -1],\n",
        "                                    [2, 0, -2],\n",
        "                                    [1, 0, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
        "            sobel_y = torch.tensor([[1, 2, 1],\n",
        "                                    [0, 0, 0],\n",
        "                                    [-1, -2, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
        "\n",
        "            Ix = FF.conv2d(prev_gray.unsqueeze(0), sobel_x, padding=1)\n",
        "            Iy = FF.conv2d(prev_gray.unsqueeze(0), sobel_y, padding=1)\n",
        "            It = next_gray.unsqueeze(0) - prev_gray.unsqueeze(0)\n",
        "\n",
        "            # Lucas–Kanade flow per-pixel in 3×3 window\n",
        "            kernel = torch.ones((1, 1, 3, 3), device=device)\n",
        "            Ixx = FF.conv2d(Ix * Ix, kernel, padding=1)\n",
        "            Iyy = FF.conv2d(Iy * Iy, kernel, padding=1)\n",
        "            Ixy = FF.conv2d(Ix * Iy, kernel, padding=1)\n",
        "            Ixt = FF.conv2d(Ix * It, kernel, padding=1)\n",
        "            Iyt = FF.conv2d(Iy * It, kernel, padding=1)\n",
        "\n",
        "            det = Ixx * Iyy - Ixy * Ixy + 1e-6\n",
        "            u = (Iyy * (-Ixt) - Ixy * (-Iyt)) / det\n",
        "            v = (-Ixy * (-Ixt) + Ixx * (-Iyt)) / det\n",
        "\n",
        "            u, v = u[0, 0], v[0, 0]  # (H, W)\n",
        "\n",
        "            # Magnitude / angle\n",
        "            magnitude = torch.sqrt(u ** 2 + v ** 2)\n",
        "            angle = torch.atan2(v, u) * (180.0 / np.pi / 2.0)\n",
        "\n",
        "            # Normalize magnitude → [0,255]\n",
        "            magnitude = 255 * (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-6)\n",
        "\n",
        "            # Divergence\n",
        "            dy_u, dx_u = torch.gradient(u)\n",
        "            dy_v, dx_v = torch.gradient(v)\n",
        "            divergence = dx_u + dy_v\n",
        "            divergence = torch.tanh(divergence)\n",
        "            divergence = ((divergence + 1.0) / 2.0) * 255\n",
        "\n",
        "            flow_channels = torch.stack([magnitude, angle, divergence], dim=0).clamp(0, 255)\n",
        "            mask_frames.append(flow_channels.to(\"cpu\", torch.uint8))\n",
        "\n",
        "        mask_frames = torch.stack(mask_frames).numpy()\n",
        "        save_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
        "        np.save(save_path, mask_frames)\n",
        "\n",
        "        del frames_tensor, mask_frames\n",
        "        gc.collect()\n",
        "\n",
        "masked_video_to_matrix(train_df_split)\n",
        "masked_video_to_matrix(val_df_split)"
      ],
      "metadata": {
        "id": "EHBcFHrN_LWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c51af8-7f64-4168-d27a-95e54ecf66ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 525/525 [05:51<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 225/225 [02:29<00:00,  1.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_matrix_path_col(df) :\n",
        "  output_dir = 'masked_video_matrices'\n",
        "  df['masked_video_matrix_path'] = None\n",
        "\n",
        "  for idx in df.index:\n",
        "      file_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
        "      if os.path.exists(file_path) :\n",
        "          df.at[idx, 'masked_video_matrix_path'] = file_path\n",
        "      else:\n",
        "          df.at[idx, 'masked_video_matrix_path'] = None\n",
        "\n",
        "add_matrix_path_col(train_df_split)\n",
        "add_matrix_path_col(val_df_split)"
      ],
      "metadata": {
        "id": "T-pH5wCoCbne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_split = train_df_split.reset_index()\n",
        "val_df_split = val_df_split.reset_index()"
      ],
      "metadata": {
        "id": "ajcc09yjuuyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLCwPSBnwBEZ"
      },
      "outputs": [],
      "source": [
        "train_df_split = train_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])\n",
        "val_df_split = val_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nh-5sN3SyGQ"
      },
      "outputs": [],
      "source": [
        "MAX_FRAMES = 10\n",
        "def pad_or_truncate(seq, max_len):\n",
        "    seq = np.array(seq)\n",
        "    if len(seq) > max_len:\n",
        "        return seq[:max_len]\n",
        "    if len(seq) < max_len:\n",
        "        last = seq[-1]\n",
        "        padding = np.repeat(last[None, ...], max_len - len(seq), axis=0)\n",
        "        return np.concatenate([seq, padding], axis=0)\n",
        "    return seq\n",
        "\n",
        "class VideoMatrixSequence(keras.utils.Sequence):\n",
        "    def __init__(self, df, batch_size=16, target_size=(224, 224), shuffle=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.columns = self.df.columns.drop(['video_matrix_path', 'masked_video_matrix_path', 'label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        tabular_data = []\n",
        "        video_data = []\n",
        "        mask_data = []\n",
        "        labels = []\n",
        "\n",
        "        for i in batch_indices :\n",
        "            row = self.df.iloc[i]\n",
        "\n",
        "            video = np.load(row['video_matrix_path'])\n",
        "            frames_resized = np.stack(video)\n",
        "            frames_resized = pad_or_truncate(frames_resized, MAX_FRAMES)\n",
        "\n",
        "            mask = np.load(row['masked_video_matrix_path'])\n",
        "            mask_frames = np.stack(mask)\n",
        "            mask_frames = pad_or_truncate(mask_frames, MAX_FRAMES)\n",
        "\n",
        "            tabular_features = [row[col] for col in self.columns]\n",
        "            tabular_data.append(tabular_features)\n",
        "\n",
        "            video_data.append(frames_resized)\n",
        "            mask_data.append(mask_frames)\n",
        "            labels.append(float(row['label']))\n",
        "\n",
        "        tabular_array = np.array(tabular_data, dtype=np.float32)\n",
        "        video_array = np.array(video_data, dtype=np.float32)\n",
        "        mask_array = np.array(mask_data, dtype=np.float32)\n",
        "        labels_array = np.array(labels, dtype=np.float32)\n",
        "        labels_array = np.expand_dims(labels_array, axis=-1)\n",
        "\n",
        "        output = (\n",
        "            {\n",
        "                'video': video_array,\n",
        "                'mask_flow': mask_array,\n",
        "                'tabular': tabular_array\n",
        "            }, labels_array\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2wX7HylfN62"
      },
      "outputs": [],
      "source": [
        "train_seq = VideoMatrixSequence(train_df_split, batch_size=16, target_size=(224, 224))\n",
        "val_seq = VideoMatrixSequence(val_df_split, batch_size=16, target_size=(224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation_pipeline(noise_std=0.1, brightness_factor=0.2, contrast_factor=0.2):\n",
        "    return tf.keras.Sequential([\n",
        "        layers.RandomBrightness(factor=brightness_factor),\n",
        "        layers.RandomContrast(factor=contrast_factor),\n",
        "        layers.GaussianNoise(noise_std)\n",
        "    ], name=\"augmentation_pipeline\")"
      ],
      "metadata": {
        "id": "j6ACOPvztaTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_cnn_net(input_shape=(3, 224, 224), dropout_rate=0.5):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Permute((2, 3, 1))(inputs)\n",
        "    x = keras.layers.Normalization(mean=0., variance=255.)(x)\n",
        "    # x = layers.TimeDistributed(augmentation_pipeline())(x)\n",
        "\n",
        "    base = applications.ResNet50(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x,\n",
        "        pooling=None\n",
        "    )\n",
        "    base.trainable = False\n",
        "\n",
        "    x = base.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    return keras.Model(inputs, x, name=\"mask_flow_backbone\")"
      ],
      "metadata": {
        "id": "uqop2UXY-BFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBRxKmhYV2yz",
        "outputId": "26c62363-c706-481f-c4e8-c2f5406d063e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5205 - auc: 0.5512 - f1: 0.5367 - loss: 0.8882 - precision: 0.5362 - recall: 0.5381\n",
            "Epoch 1: val_accuracy improved from -inf to 0.68304, saving model to best_video_model.keras\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 4s/step - accuracy: 0.5227 - auc: 0.5538 - f1: 0.5384 - loss: 0.8849 - precision: 0.5377 - recall: 0.5401 - val_accuracy: 0.6830 - val_auc: 0.7771 - val_f1: 0.7509 - val_loss: 0.5791 - val_precision: 0.6687 - val_recall: 0.8560\n",
            "Epoch 2/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6934 - auc: 0.7538 - f1: 0.6874 - loss: 0.6568 - precision: 0.6848 - recall: 0.6934\n",
            "Epoch 2: val_accuracy improved from 0.68304 to 0.75000, saving model to best_video_model.keras\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.6938 - auc: 0.7543 - f1: 0.6879 - loss: 0.6564 - precision: 0.6851 - recall: 0.6940 - val_accuracy: 0.7500 - val_auc: 0.8121 - val_f1: 0.7667 - val_loss: 0.5360 - val_precision: 0.8000 - val_recall: 0.7360\n",
            "Epoch 3/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7760 - auc: 0.8499 - f1: 0.7635 - loss: 0.4897 - precision: 0.7483 - recall: 0.7802\n",
            "Epoch 3: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.7754 - auc: 0.8497 - f1: 0.7632 - loss: 0.4899 - precision: 0.7482 - recall: 0.7796 - val_accuracy: 0.7500 - val_auc: 0.8199 - val_f1: 0.7795 - val_loss: 0.5148 - val_precision: 0.7674 - val_recall: 0.7920\n",
            "Epoch 4/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7826 - auc: 0.8662 - f1: 0.7864 - loss: 0.4741 - precision: 0.7561 - recall: 0.8236\n",
            "Epoch 4: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.7825 - auc: 0.8659 - f1: 0.7862 - loss: 0.4744 - precision: 0.7565 - recall: 0.8227 - val_accuracy: 0.7455 - val_auc: 0.8290 - val_f1: 0.7654 - val_loss: 0.5040 - val_precision: 0.7815 - val_recall: 0.7500\n",
            "Epoch 5/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7621 - auc: 0.8775 - f1: 0.7680 - loss: 0.4468 - precision: 0.7505 - recall: 0.7884\n",
            "Epoch 5: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.7621 - auc: 0.8774 - f1: 0.7679 - loss: 0.4469 - precision: 0.7503 - recall: 0.7884 - val_accuracy: 0.7455 - val_auc: 0.8243 - val_f1: 0.7467 - val_loss: 0.5387 - val_precision: 0.8317 - val_recall: 0.6774\n",
            "Epoch 6/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8358 - auc: 0.9401 - f1: 0.8297 - loss: 0.3169 - precision: 0.8180 - recall: 0.8429\n",
            "Epoch 6: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.8358 - auc: 0.9398 - f1: 0.8297 - loss: 0.3173 - precision: 0.8185 - recall: 0.8423 - val_accuracy: 0.7232 - val_auc: 0.8431 - val_f1: 0.7048 - val_loss: 0.5605 - val_precision: 0.8605 - val_recall: 0.5968\n",
            "Epoch 7/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8695 - auc: 0.9292 - f1: 0.8662 - loss: 0.3453 - precision: 0.8664 - recall: 0.8682\n",
            "Epoch 7: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.8692 - auc: 0.9293 - f1: 0.8659 - loss: 0.3450 - precision: 0.8663 - recall: 0.8676 - val_accuracy: 0.7455 - val_auc: 0.8411 - val_f1: 0.7421 - val_loss: 0.5512 - val_precision: 0.8542 - val_recall: 0.6560\n",
            "Epoch 8/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8853 - auc: 0.9514 - f1: 0.8837 - loss: 0.2778 - precision: 0.8683 - recall: 0.9003\n",
            "Epoch 8: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.8851 - auc: 0.9513 - f1: 0.8836 - loss: 0.2781 - precision: 0.8681 - recall: 0.9003 - val_accuracy: 0.7321 - val_auc: 0.8474 - val_f1: 0.7196 - val_loss: 0.5662 - val_precision: 0.8556 - val_recall: 0.6210\n",
            "Epoch 9/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8674 - auc: 0.9386 - f1: 0.8642 - loss: 0.3173 - precision: 0.8573 - recall: 0.8730\n",
            "Epoch 9: val_accuracy did not improve from 0.75000\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.8680 - auc: 0.9390 - f1: 0.8648 - loss: 0.3162 - precision: 0.8580 - recall: 0.8736 - val_accuracy: 0.7500 - val_auc: 0.8487 - val_f1: 0.7586 - val_loss: 0.5108 - val_precision: 0.8148 - val_recall: 0.7097\n",
            "Epoch 10/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8901 - auc: 0.9680 - f1: 0.8868 - loss: 0.2442 - precision: 0.8828 - recall: 0.8935\n",
            "Epoch 10: val_accuracy improved from 0.75000 to 0.75893, saving model to best_video_model.keras\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 3s/step - accuracy: 0.8902 - auc: 0.9679 - f1: 0.8870 - loss: 0.2442 - precision: 0.8829 - recall: 0.8939 - val_accuracy: 0.7589 - val_auc: 0.8491 - val_f1: 0.7787 - val_loss: 0.5061 - val_precision: 0.7917 - val_recall: 0.7661\n",
            "Epoch 11/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.8978 - auc: 0.9679 - f1: 0.8973 - loss: 0.2353 - precision: 0.9131 - recall: 0.8829\n",
            "Epoch 11: val_accuracy did not improve from 0.75893\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.8983 - auc: 0.9680 - f1: 0.8978 - loss: 0.2348 - precision: 0.9132 - recall: 0.8837 - val_accuracy: 0.7589 - val_auc: 0.8529 - val_f1: 0.7692 - val_loss: 0.5054 - val_precision: 0.8182 - val_recall: 0.7258\n",
            "Epoch 12/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9137 - auc: 0.9751 - f1: 0.9108 - loss: 0.2152 - precision: 0.9066 - recall: 0.9162\n",
            "Epoch 12: val_accuracy improved from 0.75893 to 0.78571, saving model to best_video_model.keras\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.9137 - auc: 0.9751 - f1: 0.9108 - loss: 0.2148 - precision: 0.9070 - recall: 0.9158 - val_accuracy: 0.7857 - val_auc: 0.8580 - val_f1: 0.8017 - val_loss: 0.4972 - val_precision: 0.8220 - val_recall: 0.7823\n",
            "Epoch 13/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9248 - auc: 0.9850 - f1: 0.9155 - loss: 0.2048 - precision: 0.8720 - recall: 0.9671\n",
            "Epoch 13: val_accuracy did not improve from 0.78571\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9250 - auc: 0.9848 - f1: 0.9159 - loss: 0.2045 - precision: 0.8737 - recall: 0.9661 - val_accuracy: 0.7634 - val_auc: 0.8586 - val_f1: 0.7686 - val_loss: 0.5569 - val_precision: 0.8381 - val_recall: 0.7097\n",
            "Epoch 14/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9331 - auc: 0.9801 - f1: 0.9326 - loss: 0.1859 - precision: 0.9505 - recall: 0.9159\n",
            "Epoch 14: val_accuracy did not improve from 0.78571\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9332 - auc: 0.9801 - f1: 0.9327 - loss: 0.1857 - precision: 0.9504 - recall: 0.9161 - val_accuracy: 0.7768 - val_auc: 0.8577 - val_f1: 0.8000 - val_loss: 0.5010 - val_precision: 0.7937 - val_recall: 0.8065\n",
            "Epoch 15/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9076 - auc: 0.9742 - f1: 0.9127 - loss: 0.2130 - precision: 0.9220 - recall: 0.9041\n",
            "Epoch 15: val_accuracy did not improve from 0.78571\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9083 - auc: 0.9744 - f1: 0.9132 - loss: 0.2121 - precision: 0.9221 - recall: 0.9051 - val_accuracy: 0.7679 - val_auc: 0.8565 - val_f1: 0.7851 - val_loss: 0.5172 - val_precision: 0.8120 - val_recall: 0.7600\n",
            "Epoch 16/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9289 - auc: 0.9792 - f1: 0.9283 - loss: 0.1845 - precision: 0.9225 - recall: 0.9345\n",
            "Epoch 16: val_accuracy did not improve from 0.78571\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - accuracy: 0.9293 - auc: 0.9794 - f1: 0.9287 - loss: 0.1840 - precision: 0.9229 - recall: 0.9351 - val_accuracy: 0.7634 - val_auc: 0.8652 - val_f1: 0.7686 - val_loss: 0.5504 - val_precision: 0.8462 - val_recall: 0.7040\n",
            "Epoch 17/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9543 - auc: 0.9942 - f1: 0.9536 - loss: 0.1316 - precision: 0.9526 - recall: 0.9555\n",
            "Epoch 17: val_accuracy did not improve from 0.78571\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.9544 - auc: 0.9942 - f1: 0.9537 - loss: 0.1313 - precision: 0.9525 - recall: 0.9558 - val_accuracy: 0.7768 - val_auc: 0.8600 - val_f1: 0.8047 - val_loss: 0.5292 - val_precision: 0.7803 - val_recall: 0.8306\n",
            "Epoch 18/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9647 - auc: 0.9959 - f1: 0.9628 - loss: 0.1109 - precision: 0.9663 - recall: 0.9596\n",
            "Epoch 18: val_accuracy improved from 0.78571 to 0.79018, saving model to best_video_model.keras\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 3s/step - accuracy: 0.9644 - auc: 0.9959 - f1: 0.9624 - loss: 0.1111 - precision: 0.9662 - recall: 0.9590 - val_accuracy: 0.7902 - val_auc: 0.8659 - val_f1: 0.8142 - val_loss: 0.5124 - val_precision: 0.7984 - val_recall: 0.8306\n",
            "Epoch 19/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9680 - auc: 0.9964 - f1: 0.9679 - loss: 0.1112 - precision: 0.9547 - recall: 0.9819\n",
            "Epoch 19: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9678 - auc: 0.9964 - f1: 0.9678 - loss: 0.1112 - precision: 0.9545 - recall: 0.9817 - val_accuracy: 0.7634 - val_auc: 0.8577 - val_f1: 0.8000 - val_loss: 0.5651 - val_precision: 0.7571 - val_recall: 0.8480\n",
            "Epoch 20/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9745 - auc: 0.9973 - f1: 0.9746 - loss: 0.0855 - precision: 0.9810 - recall: 0.9686\n",
            "Epoch 20: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9748 - auc: 0.9973 - f1: 0.9748 - loss: 0.0853 - precision: 0.9809 - recall: 0.9690 - val_accuracy: 0.7768 - val_auc: 0.8579 - val_f1: 0.8120 - val_loss: 0.5800 - val_precision: 0.7606 - val_recall: 0.8710\n",
            "Epoch 21/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9690 - auc: 0.9948 - f1: 0.9679 - loss: 0.1030 - precision: 0.9629 - recall: 0.9734\n",
            "Epoch 21: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9688 - auc: 0.9948 - f1: 0.9678 - loss: 0.1030 - precision: 0.9628 - recall: 0.9732 - val_accuracy: 0.7768 - val_auc: 0.8705 - val_f1: 0.8092 - val_loss: 0.5280 - val_precision: 0.7737 - val_recall: 0.8480\n",
            "Epoch 22/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9705 - auc: 0.9978 - f1: 0.9716 - loss: 0.0892 - precision: 0.9768 - recall: 0.9667\n",
            "Epoch 22: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.9705 - auc: 0.9978 - f1: 0.9716 - loss: 0.0891 - precision: 0.9767 - recall: 0.9668 - val_accuracy: 0.7634 - val_auc: 0.8545 - val_f1: 0.8044 - val_loss: 0.6025 - val_precision: 0.7466 - val_recall: 0.8720\n",
            "Epoch 23/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9868 - auc: 0.9991 - f1: 0.9866 - loss: 0.0606 - precision: 0.9809 - recall: 0.9923\n",
            "Epoch 23: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.9864 - auc: 0.9991 - f1: 0.9861 - loss: 0.0613 - precision: 0.9805 - recall: 0.9919 - val_accuracy: 0.7768 - val_auc: 0.8560 - val_f1: 0.8092 - val_loss: 0.5797 - val_precision: 0.7737 - val_recall: 0.8480\n",
            "Epoch 24/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9842 - auc: 0.9994 - f1: 0.9849 - loss: 0.0619 - precision: 0.9915 - recall: 0.9784\n",
            "Epoch 24: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9841 - auc: 0.9993 - f1: 0.9847 - loss: 0.0620 - precision: 0.9911 - recall: 0.9785 - val_accuracy: 0.7455 - val_auc: 0.8665 - val_f1: 0.7489 - val_loss: 0.6136 - val_precision: 0.8252 - val_recall: 0.6855\n",
            "Epoch 25/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9894 - auc: 0.9996 - f1: 0.9885 - loss: 0.0541 - precision: 0.9904 - recall: 0.9867\n",
            "Epoch 25: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9891 - auc: 0.9995 - f1: 0.9883 - loss: 0.0542 - precision: 0.9903 - recall: 0.9863 - val_accuracy: 0.7545 - val_auc: 0.8724 - val_f1: 0.7639 - val_loss: 0.5793 - val_precision: 0.8165 - val_recall: 0.7177\n",
            "Epoch 26/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9955 - auc: 0.9999 - f1: 0.9955 - loss: 0.0397 - precision: 0.9952 - recall: 0.9959\n",
            "Epoch 26: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.9953 - auc: 0.9999 - f1: 0.9953 - loss: 0.0400 - precision: 0.9950 - recall: 0.9956 - val_accuracy: 0.7679 - val_auc: 0.8598 - val_f1: 0.7851 - val_loss: 0.6059 - val_precision: 0.8120 - val_recall: 0.7600\n",
            "Epoch 27/40\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9859 - auc: 0.9988 - f1: 0.9857 - loss: 0.0561 - precision: 0.9930 - recall: 0.9786\n",
            "Epoch 27: val_accuracy did not improve from 0.79018\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.9860 - auc: 0.9988 - f1: 0.9858 - loss: 0.0558 - precision: 0.9929 - recall: 0.9789 - val_accuracy: 0.7679 - val_auc: 0.8582 - val_f1: 0.7869 - val_loss: 0.5936 - val_precision: 0.8067 - val_recall: 0.7680\n",
            "Epoch 27: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_TABULAR_FEATURES = 14\n",
        "\n",
        "# Inputs\n",
        "video_input = keras.Input(shape=(MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), name='video')\n",
        "mask_input = keras.Input(shape=(MAX_FRAMES, 3, IMG_SIZE, IMG_SIZE), name='mask_flow')\n",
        "tabular_input = keras.Input(shape=(NUM_TABULAR_FEATURES,), name='tabular')\n",
        "\n",
        "# Pre trained weights\n",
        "def preprocess_resnet(x):\n",
        "    return applications.resnet50.preprocess_input(x)\n",
        "\n",
        "video_x = layers.TimeDistributed(layers.Lambda(preprocess_resnet))(video_input)\n",
        "# video_x = layers.TimeDistributed(augmentation_pipeline())(video_x)\n",
        "\n",
        "# CNN Backbone\n",
        "base_cnn = applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg',\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "base_cnn.trainable = False\n",
        "\n",
        "video_features = layers.TimeDistributed(base_cnn)(video_x)\n",
        "mask_features = layers.TimeDistributed(mask_cnn_net())(mask_input)\n",
        "\n",
        "# GRU\n",
        "video_gru_out = layers.Bidirectional(layers.GRU(128, return_sequences=False))(video_features)\n",
        "video_gru_out = layers.Dropout(0.3)(video_gru_out)\n",
        "\n",
        "mask_gru_out = layers.Bidirectional(layers.GRU(128, return_sequences=False))(mask_features)\n",
        "mask_gru_out = layers.Dropout(0.3)(mask_gru_out)\n",
        "\n",
        "# Merge video and mask\n",
        "video_final = layers.Concatenate()([video_gru_out, mask_gru_out])\n",
        "\n",
        "# Tabular part\n",
        "tabular_dense = layers.Dense(64, activation='relu')(tabular_input)\n",
        "tabular_final = layers.Dropout(0.3)(tabular_dense)\n",
        "\n",
        "# Merge all\n",
        "final_merge = layers.Concatenate()([video_final, tabular_final])\n",
        "\n",
        "final_merge = layers.BatchNormalization()(final_merge)\n",
        "final_merge = layers.Dropout(0.3)(final_merge)\n",
        "\n",
        "outputs = layers.Dense(1, activation='sigmoid')(final_merge)\n",
        "\n",
        "# Final model\n",
        "model = keras.Model(\n",
        "    inputs=[video_input, mask_input, tabular_input],\n",
        "    outputs=outputs,\n",
        "    name='final_model'\n",
        ")\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        keras.metrics.AUC(name='auc'),\n",
        "        keras.metrics.FBetaScore(beta=1.0, threshold=0.5, average='macro', name='f1'),\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "cbs = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_video_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(train_seq, validation_data=val_seq, epochs=40, callbacks=cbs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEVv7ZIIdKPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19cc8398acac49f3920cc8d8b8fb2cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_686963f711f54183932fcb2ef3754da8",
              "IPY_MODEL_25f4dd9d03494ac2a7a1fd137fa101cf",
              "IPY_MODEL_8f20c34caf1c464386c23af0495a0f16"
            ],
            "layout": "IPY_MODEL_d64d147a7f0b445aa57d36ffd8b6e5f7"
          }
        },
        "686963f711f54183932fcb2ef3754da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77a5029d588f40468c2809953875b363",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2f2e12b7114361a3e20c3fa316d635",
            "value": "Resolving data files: 100%"
          }
        },
        "25f4dd9d03494ac2a7a1fd137fa101cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d342f4233eb747c9b75f0e98f553f7cf",
            "max": 1502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a1ed29c5a8346e9943fbae1cab5d296",
            "value": 1502
          }
        },
        "8f20c34caf1c464386c23af0495a0f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16f56097904a4768bea9a6a8f2be432b",
            "placeholder": "​",
            "style": "IPY_MODEL_ccf5ecbc418b4ac3a1b5ad419fa09806",
            "value": " 1502/1502 [00:00&lt;00:00,  6.22it/s]"
          }
        },
        "d64d147a7f0b445aa57d36ffd8b6e5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77a5029d588f40468c2809953875b363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2f2e12b7114361a3e20c3fa316d635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d342f4233eb747c9b75f0e98f553f7cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a1ed29c5a8346e9943fbae1cab5d296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16f56097904a4768bea9a6a8f2be432b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf5ecbc418b4ac3a1b5ad419fa09806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38cb0c5f4766446f9e4a25420ccafd13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_acc0ddbe2303485d87402060d434178b",
              "IPY_MODEL_c294f991273940d3bfcd18092527ba3c",
              "IPY_MODEL_f125ff7aebd64eb093ba90c989f4f029"
            ],
            "layout": "IPY_MODEL_1a1f0a3e943c43ec8828707d50b1096e"
          }
        },
        "acc0ddbe2303485d87402060d434178b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f4bf19679744618b83d87062e044088",
            "placeholder": "​",
            "style": "IPY_MODEL_29058c00b5ee47ddabebdf1c78f648c9",
            "value": "Resolving data files: 100%"
          }
        },
        "c294f991273940d3bfcd18092527ba3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c66a59f2eee4d02bb296fe9ad18264a",
            "max": 1348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b13b948fbcb4ceabd7c5388105e5e90",
            "value": 1348
          }
        },
        "f125ff7aebd64eb093ba90c989f4f029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa9acbf49e0400d896af6d400b2bc1f",
            "placeholder": "​",
            "style": "IPY_MODEL_e5784dbd1616476b8c288ea7c37daa8a",
            "value": " 1348/1348 [00:00&lt;00:00, 17426.98it/s]"
          }
        },
        "1a1f0a3e943c43ec8828707d50b1096e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4bf19679744618b83d87062e044088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29058c00b5ee47ddabebdf1c78f648c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c66a59f2eee4d02bb296fe9ad18264a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b13b948fbcb4ceabd7c5388105e5e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa9acbf49e0400d896af6d400b2bc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5784dbd1616476b8c288ea7c37daa8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
