{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "56lwEQVts72M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce0a2f2-8213-4c72-9cf4-ae5cb046e8b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from moviepy.editor import VideoFileClip\n",
        "from keras import layers, applications\n",
        "from datasets import load_dataset\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms.functional as F\n",
        "import torch.nn.functional as FF\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2, os, gc, torch"
      ],
      "metadata": {
        "id": "r5K5XLrUGMki",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f1f89e-9b76-4521-929e-a61a6cb992f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:367: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  rotation_lines = [l for l in lines if 'rotate          :' in l and re.search('\\d+$', l)]\n",
            "/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:370: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  match = re.search('\\d+$', rotation_line)\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4d77b6127cf54f64be3dbfecf24cf309",
            "b1bfcace98e44f4a9d1a65d0071d9c71",
            "f240a9b9ee6f4bd89e6e2ca62f418877",
            "44dfc079ec014f6083bf0960700e0c2b",
            "3fc9c266134147b6bbfe2dfcb891a0cf",
            "adf1ca8df4294a438820c0703317ddde",
            "70e9000eecb64eea95678b013fdc87f1",
            "c94467208e3c48c1bb1c04758d90702a",
            "292ee6f4194f4182810976dc58a4e3a1",
            "074d87f7bf6749fe85ba8a3915adb43e",
            "7960f6a732ee49f6abf068e6e30ef486",
            "71727f71b9a94063b564d93911e15e1a",
            "c4857ae76bc64dc9843595bee8e2e173",
            "c905fa6c96234702bc3868f610e6e3cc",
            "46f94cff9444478b8c997aeade4653c6",
            "425741742a7649b8872333d39fc54eec",
            "871b7e387c5b42e1a06a163e1233015c",
            "b2fa8d1b1da2499ab79d47a2865fc1b5",
            "19ab0beac42c4ddca446db7a784e1639",
            "20f23d2cd92541089265cd9eab65184a",
            "6cec43ee9f384a09b08f91fdcd1a1d47",
            "2fe19fb7941640bf88f2da87189986fa"
          ]
        },
        "id": "299ub9AbtTYA",
        "outputId": "ad00ef04-2dcb-4990-bc3a-6bcfa706bb82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1502 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d77b6127cf54f64be3dbfecf24cf309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/1348 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71727f71b9a94063b564d93911e15e1a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "ds = load_dataset('nexar-ai/nexar_collision_prediction', token='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', split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1TwOr4Ad_tsC"
      },
      "outputs": [],
      "source": [
        "n = round(ds.num_rows * 0.6)\n",
        "subset = ds.shuffle(seed=1404).select(range(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UdBa8Xtvtcs7"
      },
      "outputs": [],
      "source": [
        "train_df = subset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7f11eX2VbE_T"
      },
      "outputs": [],
      "source": [
        "train_df['label'] = train_df['time_of_event'].apply(lambda x: 0 if pd.isna(x) or x is None else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mbF_a676cyPI"
      },
      "outputs": [],
      "source": [
        "train_df = pd.get_dummies(\n",
        "    train_df,\n",
        "    columns=['light_conditions', 'weather', 'scene'],\n",
        "    prefix=['light', 'weather', 'scene'],\n",
        "    dtype=int\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_split, val_df_split = train_test_split(train_df, test_size=0.3, random_state=1404)"
      ],
      "metadata": {
        "id": "7781WD8Whpv9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = 'video_matrices'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def resize_on_gpu(frames):\n",
        "    # Convert (N, H, W, C) -> (N, C, H, W)\n",
        "    frames_tensor = torch.from_numpy(frames).permute(0, 3, 1, 2).float().to(device)\n",
        "    # Resize to (224, 224)\n",
        "    resized = F.resize(frames_tensor, [224, 224])\n",
        "    # Back to (N, H, W, C)\n",
        "    return resized.permute(0, 2, 3, 1).cpu().numpy()\n",
        "\n",
        "def video_to_matrix(video_df, is_train=True, frame_interval=6):\n",
        "    for idx, row in tqdm(video_df.iterrows(), total=len(video_df)):\n",
        "        video_path = row['video']['path']\n",
        "        time_of_event = row['time_of_event']\n",
        "        label = row['label']\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if not cap.isOpened():\n",
        "            print(f'[WARN] Cannot open video at index {idx}: {video_path}')\n",
        "            continue\n",
        "\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        if fps == 0 or fps is None:\n",
        "            print(f'[WARN] FPS is zero at index {idx}')\n",
        "            cap.release()\n",
        "            continue\n",
        "\n",
        "        total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "        duration = total_frames / fps\n",
        "\n",
        "        start_time = duration - 3\n",
        "        end_time = duration\n",
        "\n",
        "        start_frame = int(start_time * fps)\n",
        "        end_frame = int(end_time * fps)\n",
        "\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "\n",
        "        frames = []\n",
        "        frame_index = start_frame\n",
        "\n",
        "        while frame_index <= end_frame:\n",
        "            success, frame = cap.read()\n",
        "            if not success:\n",
        "                break\n",
        "            if frame_index % frame_interval == 0:\n",
        "                frames.append(frame)\n",
        "            frame_index += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        if frames:\n",
        "            frames = np.stack(frames)  # (N, H, W, 3)\n",
        "            video_matrix = resize_on_gpu(frames)  # GPU accelerated resize\n",
        "            save_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
        "            np.save(save_path, video_matrix)\n",
        "\n",
        "        del frames\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "# Run it\n",
        "video_to_matrix(train_df_split)\n",
        "video_to_matrix(val_df_split, is_train=False)"
      ],
      "metadata": {
        "id": "81feoejPGdHd",
        "outputId": "95369acf-b615-4d70-b35f-bf5b4397364e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 91/630 [02:36<15:24,  1.71s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_tHhlu4acKv"
      },
      "outputs": [],
      "source": [
        "def add_matrix_path_col(df) :\n",
        "  output_dir = 'video_matrices'\n",
        "  df['video_matrix_path'] = None\n",
        "\n",
        "  for idx in df.index:\n",
        "      file_path = os.path.join(output_dir, f'video_{idx}.npy')\n",
        "      if os.path.exists(file_path):\n",
        "          df.at[idx, 'video_matrix_path'] = file_path\n",
        "      else:\n",
        "          df.at[idx, 'video_matrix_path'] = None\n",
        "\n",
        "add_matrix_path_col(train_df_split)\n",
        "add_matrix_path_col(val_df_split)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_video_to_matrix(df):\n",
        "    output_dir = 'masked_video_matrices'\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    for idx, video in tqdm(df.iterrows(), total=len(df)):\n",
        "        video_path = '/content/' + video['video_matrix_path']\n",
        "        video_frames = np.load(video_path)  # (N, H, W, 3)\n",
        "        frames_tensor = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float().to(device) / 255.0\n",
        "        num_frames = frames_tensor.shape[0]\n",
        "\n",
        "        mask_frames = []\n",
        "\n",
        "        # Convert to grayscale on GPU\n",
        "        gray = 0.2989 * frames_tensor[:, 0] + 0.5870 * frames_tensor[:, 1] + 0.1140 * frames_tensor[:, 2]\n",
        "\n",
        "        for i in range(1, num_frames):\n",
        "            prev_gray = gray[i - 1:i]  # (1, H, W)\n",
        "            next_gray = gray[i:i + 1]\n",
        "\n",
        "            # === Optical flow approximation using spatial gradients ===\n",
        "            # Compute gradients with Sobel kernels\n",
        "            sobel_x = torch.tensor([[1, 0, -1],\n",
        "                                    [2, 0, -2],\n",
        "                                    [1, 0, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
        "            sobel_y = torch.tensor([[1, 2, 1],\n",
        "                                    [0, 0, 0],\n",
        "                                    [-1, -2, -1]], dtype=torch.float32, device=device).view(1, 1, 3, 3)\n",
        "\n",
        "            Ix = FF.conv2d(prev_gray.unsqueeze(0), sobel_x, padding=1)\n",
        "            Iy = FF.conv2d(prev_gray.unsqueeze(0), sobel_y, padding=1)\n",
        "            It = next_gray.unsqueeze(0) - prev_gray.unsqueeze(0)\n",
        "\n",
        "            # Lucas–Kanade flow per-pixel in 3×3 window\n",
        "            kernel = torch.ones((1, 1, 3, 3), device=device)\n",
        "            Ixx = FF.conv2d(Ix * Ix, kernel, padding=1)\n",
        "            Iyy = FF.conv2d(Iy * Iy, kernel, padding=1)\n",
        "            Ixy = FF.conv2d(Ix * Iy, kernel, padding=1)\n",
        "            Ixt = FF.conv2d(Ix * It, kernel, padding=1)\n",
        "            Iyt = FF.conv2d(Iy * It, kernel, padding=1)\n",
        "\n",
        "            det = Ixx * Iyy - Ixy * Ixy + 1e-6\n",
        "            u = (Iyy * (-Ixt) - Ixy * (-Iyt)) / det\n",
        "            v = (-Ixy * (-Ixt) + Ixx * (-Iyt)) / det\n",
        "\n",
        "            u, v = u[0, 0], v[0, 0]  # (H, W)\n",
        "\n",
        "            # Magnitude / angle\n",
        "            magnitude = torch.sqrt(u ** 2 + v ** 2)\n",
        "            angle = torch.atan2(v, u) * (180.0 / np.pi / 2.0)\n",
        "\n",
        "            # Normalize magnitude → [0,255]\n",
        "            magnitude = 255 * (magnitude - magnitude.min()) / (magnitude.max() - magnitude.min() + 1e-6)\n",
        "\n",
        "            # Divergence\n",
        "            dy_u, dx_u = torch.gradient(u)\n",
        "            dy_v, dx_v = torch.gradient(v)\n",
        "            divergence = dx_u + dy_v\n",
        "            divergence = torch.tanh(divergence)\n",
        "            divergence = ((divergence + 1.0) / 2.0) * 255\n",
        "\n",
        "            flow_channels = torch.stack([magnitude, angle, divergence], dim=0).clamp(0, 255)\n",
        "            mask_frames.append(flow_channels.to(\"cpu\", torch.uint8))\n",
        "\n",
        "        mask_frames = torch.stack(mask_frames).numpy()\n",
        "        save_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
        "        np.save(save_path, mask_frames)\n",
        "\n",
        "        del frames_tensor, mask_frames\n",
        "        gc.collect()\n",
        "\n",
        "masked_video_to_matrix(train_df_split)\n",
        "masked_video_to_matrix(val_df_split)"
      ],
      "metadata": {
        "id": "EHBcFHrN_LWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_matrix_path_col(df) :\n",
        "  output_dir = 'masked_video_matrices'\n",
        "  df['masked_video_matrix_path'] = None\n",
        "\n",
        "  for idx in df.index:\n",
        "      file_path = os.path.join(output_dir, f'masked_video_{idx}.npy')\n",
        "      if os.path.exists(file_path) :\n",
        "          df.at[idx, 'masked_video_matrix_path'] = file_path\n",
        "      else:\n",
        "          df.at[idx, 'masked_video_matrix_path'] = None\n",
        "\n",
        "add_matrix_path_col(train_df_split)\n",
        "add_matrix_path_col(val_df_split)"
      ],
      "metadata": {
        "id": "T-pH5wCoCbne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_split = train_df_split.reset_index()\n",
        "val_df_split = val_df_split.reset_index()"
      ],
      "metadata": {
        "id": "ajcc09yjuuyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLCwPSBnwBEZ"
      },
      "outputs": [],
      "source": [
        "train_df_split = train_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])\n",
        "val_df_split = val_df_split.drop(columns=['video', 'time_of_event', 'time_of_alert', 'time_to_accident', 'index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Nh-5sN3SyGQ"
      },
      "outputs": [],
      "source": [
        "MAX_FRAMES = 5\n",
        "def pad_or_truncate(seq, max_len):\n",
        "    seq = np.array(seq)\n",
        "    if len(seq) > max_len:\n",
        "        return seq[:max_len]\n",
        "    if len(seq) < max_len:\n",
        "        last = seq[-1]\n",
        "        padding = np.repeat(last[None, ...], max_len - len(seq), axis=0)\n",
        "        return np.concatenate([seq, padding], axis=0)\n",
        "    return seq\n",
        "\n",
        "class VideoMatrixSequence(keras.utils.Sequence):\n",
        "    def __init__(self, df, batch_size=16, target_size=(224, 224), shuffle=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.columns = self.df.columns.drop(['video_matrix_path', 'masked_video_matrix_path', 'label'])\n",
        "        self.indices = np.arange(len(self.df))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        tabular_data = []\n",
        "        video_data = []\n",
        "        mask_data = []\n",
        "        labels = []\n",
        "\n",
        "        for i in batch_indices :\n",
        "            row = self.df.iloc[i]\n",
        "\n",
        "            video = np.load(row['video_matrix_path'])\n",
        "            frames_resized = np.stack(video)\n",
        "            frames_resized = pad_or_truncate(frames_resized, MAX_FRAMES)\n",
        "\n",
        "            mask = np.load(row['masked_video_matrix_path'])\n",
        "\n",
        "            resized_mask_frames = []\n",
        "            for frame in mask:\n",
        "                ch, h, w = frame.shape\n",
        "                resized = np.stack([\n",
        "                    cv2.resize(frame[c], (self.target_size[0], self.target_size[1]))\n",
        "                    for c in range(ch)\n",
        "                ], axis=0)\n",
        "                resized_mask_frames.append(resized)\n",
        "\n",
        "            mask_frames = np.stack(resized_mask_frames)\n",
        "            mask_frames = pad_or_truncate(mask_frames, MAX_FRAMES)\n",
        "\n",
        "            tabular_features = [row[col] for col in self.columns]\n",
        "            tabular_data.append(tabular_features)\n",
        "\n",
        "            video_data.append(frames_resized)\n",
        "            mask_data.append(mask_frames)\n",
        "            labels.append(float(row['label']))\n",
        "\n",
        "        tabular_array = np.array(tabular_data, dtype=np.float32)\n",
        "        video_array = np.array(video_data, dtype=np.float32)\n",
        "        mask_array = np.array(mask_data, dtype=np.float32)\n",
        "        labels_array = np.array(labels, dtype=np.float32)\n",
        "        labels_array = np.expand_dims(labels_array, axis=-1)\n",
        "\n",
        "        output = (\n",
        "            {\n",
        "                'video': video_array,\n",
        "                'mask_flow': mask_array,\n",
        "                'tabular': tabular_array\n",
        "            }, labels_array\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2wX7HylfN62"
      },
      "outputs": [],
      "source": [
        "train_seq = VideoMatrixSequence(train_df_split, batch_size=8, target_size=(224, 224))\n",
        "val_seq = VideoMatrixSequence(val_df_split, batch_size=8, target_size=(224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation_pipeline(noise_std=0.05, brightness_factor=0.2, contrast_factor=0.2):\n",
        "    return tf.keras.Sequential([\n",
        "        layers.RandomBrightness(factor=brightness_factor),\n",
        "        layers.RandomContrast(factor=contrast_factor),\n",
        "        layers.GaussianNoise(noise_std)\n",
        "    ], name=\"augmentation_pipeline\")"
      ],
      "metadata": {
        "id": "j6ACOPvztaTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_cnn_net(input_shape=(3, 224, 224), dropout_rate=0.3):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Permute((2, 3, 1))(inputs)\n",
        "    x = layers.TimeDistributed(augmentation_pipeline())(x)\n",
        "\n",
        "    base = applications.ResNet50(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=x,\n",
        "        pooling=None\n",
        "    )\n",
        "\n",
        "    for layer in base.layers[-2:]:\n",
        "      layer.trainable = True\n",
        "\n",
        "    x = base.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    return keras.Model(inputs, x, name=\"efficientnet_flow_backbone\")"
      ],
      "metadata": {
        "id": "uqop2UXY-BFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBRxKmhYV2yz",
        "outputId": "e9c6cd4c-83df-4a8f-e04b-3b878f24416d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5013 - auc: 0.4891 - f1: 0.4943 - loss: 0.9843 - precision: 0.4748 - recall: 0.5166\n",
            "Epoch 1: val_accuracy improved from -inf to 0.56641, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 5s/step - accuracy: 0.5018 - auc: 0.4895 - f1: 0.4949 - loss: 0.9836 - precision: 0.4758 - recall: 0.5168 - val_accuracy: 0.5664 - val_auc: 0.6269 - val_f1: 0.5747 - val_loss: 0.6584 - val_precision: 0.5952 - val_recall: 0.5556\n",
            "Epoch 2/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.5827 - auc: 0.6176 - f1: 0.5637 - loss: 0.8164 - precision: 0.5261 - recall: 0.6103\n",
            "Epoch 2: val_accuracy improved from 0.56641 to 0.61328, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 4s/step - accuracy: 0.5826 - auc: 0.6176 - f1: 0.5641 - loss: 0.8160 - precision: 0.5273 - recall: 0.6096 - val_accuracy: 0.6133 - val_auc: 0.6698 - val_f1: 0.5959 - val_loss: 0.6421 - val_precision: 0.6636 - val_recall: 0.5407\n",
            "Epoch 3/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6298 - auc: 0.6751 - f1: 0.6478 - loss: 0.7091 - precision: 0.6323 - recall: 0.6661\n",
            "Epoch 3: val_accuracy improved from 0.61328 to 0.62109, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 4s/step - accuracy: 0.6301 - auc: 0.6756 - f1: 0.6477 - loss: 0.7087 - precision: 0.6325 - recall: 0.6656 - val_accuracy: 0.6211 - val_auc: 0.6975 - val_f1: 0.6284 - val_loss: 0.6233 - val_precision: 0.6508 - val_recall: 0.6074\n",
            "Epoch 4/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6502 - auc: 0.7085 - f1: 0.6631 - loss: 0.6630 - precision: 0.6625 - recall: 0.6645\n",
            "Epoch 4: val_accuracy improved from 0.62109 to 0.63281, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 5s/step - accuracy: 0.6503 - auc: 0.7085 - f1: 0.6629 - loss: 0.6629 - precision: 0.6624 - recall: 0.6642 - val_accuracy: 0.6328 - val_auc: 0.7082 - val_f1: 0.6299 - val_loss: 0.6208 - val_precision: 0.6667 - val_recall: 0.5970\n",
            "Epoch 5/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6931 - auc: 0.7435 - f1: 0.6662 - loss: 0.6404 - precision: 0.6519 - recall: 0.6843\n",
            "Epoch 5: val_accuracy did not improve from 0.63281\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 3s/step - accuracy: 0.6931 - auc: 0.7434 - f1: 0.6667 - loss: 0.6404 - precision: 0.6530 - recall: 0.6842 - val_accuracy: 0.6250 - val_auc: 0.7026 - val_f1: 0.6336 - val_loss: 0.6361 - val_precision: 0.6640 - val_recall: 0.6058\n",
            "Epoch 6/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7121 - auc: 0.7805 - f1: 0.7120 - loss: 0.5932 - precision: 0.7609 - recall: 0.6719\n",
            "Epoch 6: val_accuracy did not improve from 0.63281\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 3s/step - accuracy: 0.7126 - auc: 0.7810 - f1: 0.7125 - loss: 0.5924 - precision: 0.7602 - recall: 0.6733 - val_accuracy: 0.6211 - val_auc: 0.7001 - val_f1: 0.6226 - val_loss: 0.6424 - val_precision: 0.6400 - val_recall: 0.6061\n",
            "Epoch 7/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.6814 - auc: 0.7809 - f1: 0.6763 - loss: 0.5820 - precision: 0.6689 - recall: 0.6872\n",
            "Epoch 7: val_accuracy improved from 0.63281 to 0.64453, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 5s/step - accuracy: 0.6817 - auc: 0.7812 - f1: 0.6766 - loss: 0.5815 - precision: 0.6695 - recall: 0.6872 - val_accuracy: 0.6445 - val_auc: 0.7186 - val_f1: 0.6316 - val_loss: 0.6376 - val_precision: 0.6724 - val_recall: 0.5954\n",
            "Epoch 8/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.7293 - auc: 0.8213 - f1: 0.7246 - loss: 0.5492 - precision: 0.7275 - recall: 0.7229\n",
            "Epoch 8: val_accuracy improved from 0.64453 to 0.66016, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 5s/step - accuracy: 0.7305 - auc: 0.8224 - f1: 0.7258 - loss: 0.5471 - precision: 0.7287 - recall: 0.7243 - val_accuracy: 0.6602 - val_auc: 0.7239 - val_f1: 0.6534 - val_loss: 0.6441 - val_precision: 0.6891 - val_recall: 0.6212\n",
            "Epoch 9/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8172 - auc: 0.8854 - f1: 0.8179 - loss: 0.4280 - precision: 0.8150 - recall: 0.8216\n",
            "Epoch 9: val_accuracy did not improve from 0.66016\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 3s/step - accuracy: 0.8174 - auc: 0.8858 - f1: 0.8181 - loss: 0.4273 - precision: 0.8152 - recall: 0.8217 - val_accuracy: 0.6367 - val_auc: 0.7151 - val_f1: 0.6324 - val_loss: 0.6601 - val_precision: 0.6723 - val_recall: 0.5970\n",
            "Epoch 10/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8412 - auc: 0.9264 - f1: 0.8471 - loss: 0.3445 - precision: 0.8482 - recall: 0.8463\n",
            "Epoch 10: val_accuracy improved from 0.66016 to 0.67188, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 5s/step - accuracy: 0.8414 - auc: 0.9265 - f1: 0.8471 - loss: 0.3445 - precision: 0.8482 - recall: 0.8463 - val_accuracy: 0.6719 - val_auc: 0.7281 - val_f1: 0.6794 - val_loss: 0.6740 - val_precision: 0.7120 - val_recall: 0.6496\n",
            "Epoch 11/20\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.8451 - auc: 0.9293 - f1: 0.8378 - loss: 0.3397 - precision: 0.8227 - recall: 0.8611\n",
            "Epoch 11: val_accuracy improved from 0.67188 to 0.67578, saving model to best_video_model.keras\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 5s/step - accuracy: 0.8459 - auc: 0.9297 - f1: 0.8388 - loss: 0.3387 - precision: 0.8242 - recall: 0.8612 - val_accuracy: 0.6758 - val_auc: 0.7240 - val_f1: 0.6795 - val_loss: 0.6894 - val_precision: 0.7154 - val_recall: 0.6471\n",
            "Epoch 11: early stopping\n",
            "Restoring model weights from the end of the best epoch: 4.\n"
          ]
        }
      ],
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_TABULAR_FEATURES = 14\n",
        "\n",
        "# Inputs\n",
        "video_input = keras.Input(shape=(MAX_FRAMES, IMG_SIZE, IMG_SIZE, 3), name='video')\n",
        "mask_input = keras.Input(shape=(MAX_FRAMES, 3, IMG_SIZE, IMG_SIZE), name='mask_flow')\n",
        "tabular_input = keras.Input(shape=(NUM_TABULAR_FEATURES,), name='tabular')\n",
        "\n",
        "# Pre trained weights\n",
        "def preprocess_resnet(x):\n",
        "    return applications.resnet50.preprocess_input(x)\n",
        "\n",
        "video_x = layers.TimeDistributed(layers.Lambda(preprocess_resnet))(video_input)\n",
        "video_x = layers.TimeDistributed(augmentation_pipeline())(video_x)\n",
        "\n",
        "# CNN Backbone\n",
        "base_cnn = applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    pooling='avg',\n",
        "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
        ")\n",
        "\n",
        "mask_cnn = mask_cnn_net()\n",
        "\n",
        "mask_cnn.trainable = False\n",
        "\n",
        "video_features = layers.TimeDistributed(base_cnn)(video_x)\n",
        "mask_features = layers.TimeDistributed(mask_cnn)(mask_input)\n",
        "\n",
        "# Merge video and mask\n",
        "merged_frames = layers.Concatenate()([video_features, mask_features])\n",
        "\n",
        "# GRU\n",
        "gru_out = layers.Bidirectional(layers.GRU(256, return_sequences=False))(merged_frames)\n",
        "video_final = layers.Dropout(0.5)(gru_out)\n",
        "\n",
        "# Tabular part\n",
        "tabular_dense = layers.Dense(64, activation='relu')(tabular_input)\n",
        "tabular_final = layers.Dropout(0.3)(tabular_dense)\n",
        "\n",
        "# Merge all\n",
        "final_merge = layers.Concatenate()([video_final, tabular_final])\n",
        "\n",
        "final_merge = layers.BatchNormalization()(final_merge)\n",
        "final_merge = layers.Dropout(0.3)(final_merge)\n",
        "\n",
        "outputs = layers.Dense(1, activation='sigmoid')(final_merge)\n",
        "\n",
        "# Final model\n",
        "model = keras.Model(\n",
        "    inputs=[video_input, mask_input, tabular_input],\n",
        "    outputs=outputs,\n",
        "    name='final_model'\n",
        ")\n",
        "\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        keras.metrics.AUC(name='auc'),\n",
        "        keras.metrics.FBetaScore(beta=1.0, threshold=0.5, average='macro', name='f1'),\n",
        "        keras.metrics.Precision(name='precision'),\n",
        "        keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "cbs = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=7,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_video_model.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(train_seq, validation_data=val_seq, epochs=20, callbacks=cbs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEVv7ZIIdKPl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d77b6127cf54f64be3dbfecf24cf309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1bfcace98e44f4a9d1a65d0071d9c71",
              "IPY_MODEL_f240a9b9ee6f4bd89e6e2ca62f418877",
              "IPY_MODEL_44dfc079ec014f6083bf0960700e0c2b"
            ],
            "layout": "IPY_MODEL_3fc9c266134147b6bbfe2dfcb891a0cf"
          }
        },
        "b1bfcace98e44f4a9d1a65d0071d9c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf1ca8df4294a438820c0703317ddde",
            "placeholder": "​",
            "style": "IPY_MODEL_70e9000eecb64eea95678b013fdc87f1",
            "value": "Resolving data files: 100%"
          }
        },
        "f240a9b9ee6f4bd89e6e2ca62f418877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c94467208e3c48c1bb1c04758d90702a",
            "max": 1502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_292ee6f4194f4182810976dc58a4e3a1",
            "value": 1502
          }
        },
        "44dfc079ec014f6083bf0960700e0c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_074d87f7bf6749fe85ba8a3915adb43e",
            "placeholder": "​",
            "style": "IPY_MODEL_7960f6a732ee49f6abf068e6e30ef486",
            "value": " 1502/1502 [00:00&lt;00:00, 120.46it/s]"
          }
        },
        "3fc9c266134147b6bbfe2dfcb891a0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf1ca8df4294a438820c0703317ddde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e9000eecb64eea95678b013fdc87f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c94467208e3c48c1bb1c04758d90702a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "292ee6f4194f4182810976dc58a4e3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "074d87f7bf6749fe85ba8a3915adb43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7960f6a732ee49f6abf068e6e30ef486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71727f71b9a94063b564d93911e15e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4857ae76bc64dc9843595bee8e2e173",
              "IPY_MODEL_c905fa6c96234702bc3868f610e6e3cc",
              "IPY_MODEL_46f94cff9444478b8c997aeade4653c6"
            ],
            "layout": "IPY_MODEL_425741742a7649b8872333d39fc54eec"
          }
        },
        "c4857ae76bc64dc9843595bee8e2e173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_871b7e387c5b42e1a06a163e1233015c",
            "placeholder": "​",
            "style": "IPY_MODEL_b2fa8d1b1da2499ab79d47a2865fc1b5",
            "value": "Resolving data files: 100%"
          }
        },
        "c905fa6c96234702bc3868f610e6e3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ab0beac42c4ddca446db7a784e1639",
            "max": 1348,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20f23d2cd92541089265cd9eab65184a",
            "value": 1348
          }
        },
        "46f94cff9444478b8c997aeade4653c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cec43ee9f384a09b08f91fdcd1a1d47",
            "placeholder": "​",
            "style": "IPY_MODEL_2fe19fb7941640bf88f2da87189986fa",
            "value": " 1348/1348 [00:00&lt;00:00, 19593.23it/s]"
          }
        },
        "425741742a7649b8872333d39fc54eec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "871b7e387c5b42e1a06a163e1233015c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fa8d1b1da2499ab79d47a2865fc1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19ab0beac42c4ddca446db7a784e1639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f23d2cd92541089265cd9eab65184a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cec43ee9f384a09b08f91fdcd1a1d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe19fb7941640bf88f2da87189986fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}